<html>
<head>
<script>
	window.MathJax = {
		tex: {
		inlineMath: [['$', '$'], ['\\(', '\\)']],
		displayMath: [['$$', '$$'], ['\\[', '\\]']]
		}
	};
	</script>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<link rel="shortcut icon" href="images/icon.ico">
<style type="text/css">
	body {
		background-color: #f5f9ff;
	}

	/* Hide both math displays initially, will display based on JS detection */
  .mathjax-mobile, .mathml-non-mobile { display: none; }

  /* Show the MathML content by default on non-mobile devices */
  .show-mathml .mathml-non-mobile { display: block; }
  .show-mathjax .mathjax-mobile { display: block; }

	.content-margin-container {
		display: flex;
		width: 100%; /* Ensure the container is full width */
		justify-content: left; /* Horizontally centers the children in the container */
		align-items: center;  /* Vertically centers the children in the container */
	}
	.main-content-block {
		width: 70%; /* Change this percentage as needed */
    max-width: 1100px; /* Optional: Maximum width */
		background-color: #fff;
		border-left: 1px solid #DDD;
		border-right: 1px solid #DDD;
		padding: 8px 8px 8px 8px;
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
	}
	.margin-left-block {
			font-size: 14px;
			width: 15%; /* Change this percentage as needed */
			max-width: 130px; /* Optional: Maximum width */
			position: relative;
			margin-left: 10px;
			text-align: left;
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
			padding: 5px;
	}
	.margin-right-block {
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
			font-size: 14px;
			width: 25%; /* Change this percentage as needed */
			max-width: 256px; /* Optional: Maximum width */
			position: relative;
			text-align: left;
			padding: 10px;  /* Optional: Adds padding inside the caption */
	}

	img {
			max-width: 100%; /* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
	}
	.my-video {
			max-width: 100%; /* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
	}
	/* Hide both video displays initially, will display based on JS detection */
  .vid-mobile, .vid-non-mobile { display: none; }

  /* Show the video content by default on non-mobile devices */
  .show-vid-mobile .vid-mobile { display: block; }
  .show-vid-non-mobile .vid-non-mobile { display: block; }

	a:link,a:visited
	{
		color: #0e7862; /*#1367a7;*/
		text-decoration: none;
	}
	a:hover {
		color: #24b597; /*#208799;*/
	}

	h1 {
		font-size: 18px;
		margin-top: 4px;
		margin-bottom: 10px;
	}

	table.header {
    font-weight: 300;
    font-size: 17px;
    flex-grow: 1;
		width: 70%;
    max-width: calc(100% - 290px); /* Adjust according to the width of .paper-code-tab */
	}
	table td, table td * {
	    vertical-align: middle;
	    position: relative;
	}
	table.paper-code-tab {
	    flex-shrink: 0;
	    margin-left: 8px;
	    margin-top: 8px;
	    padding: 0px 0px 0px 8px;
	    width: 290px;
	    height: 150px;
	}

	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	hr {
    height: 1px; /* Sets the height of the line to 1 pixel */
    border: none; /* Removes the default border */
    background-color: #DDD; /* Sets the line color to black */
  }

	div.hypothesis {
		width: 80%;
		background-color: #EEE;
		border: 1px solid black;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
		font-family: Courier;
		font-size: 18px;
		text-align: center;
		margin: auto;
		padding: 16px 16px 16px 16px;
	}

	div.citation {
    font-size: 0.8em;
    background-color:#fff;
    padding: 10px;
		height: 200px;
  }

	.fade-in-inline {
		position: absolute;
		text-align: center;
		margin: auto;
		-webkit-mask-image: linear-gradient(to right,
																			transparent 0%,
																			transparent 40%,
																			black 50%,
																			black 90%,
																			transparent 100%);
		mask-image: linear-gradient(to right,
																transparent 0%,
																transparent 40%,
																black 50%,
																black 90%,
																transparent 100%);
		-webkit-mask-size: 8000% 100%;
		mask-size: 8000% 100%;
		animation-name: sweepMask;
		animation-duration: 4s;
		animation-iteration-count: infinite;
		animation-timing-function: linear;
		animation-delay: -1s;
	}

	.fade-in2-inline {
			animation-delay: 1s;
	}

	.inline-div {
			position: relative;
	    display: inline-block; /* Makes both the div and paragraph inline-block elements */
	    vertical-align: top; /* Aligns them at the top, you can adjust this to middle, bottom, etc., based on your needs */
	    width: 50px; /* Optional: Adds space between the div and the paragraph */
	}
    .algorithm-container {
		max-width: 600px;
		margin: 0 auto; /* Center the container */
		padding: 20px;
		background-color: #f9f9f9;
		border: 1px solid #ddd;
		border-radius: 5px;
		text-align: left; /* Ensure text within the container is left-aligned */
		display: block; /* Prevent flexbox issues */
	}
	.title {
		font-weight: bold;
		margin-bottom: 10px;
	}
	.code {
		font-family: "Courier New", Courier, monospace;
		background: #f4f4f4;
		padding: 10px;
		border-radius: 5px;
		border: 1px solid #ccc;
		text-align: left; /* Explicitly set text alignment to left */
		display: block;
	}
    .formula {
            font-family: "Courier New", monospace;
            background-color: #f4f4f4;
            padding: 10px;
            border-radius: 5px;
            border: 1px solid #ccc;
            display: inline-block;
        }

    li {
            margin-bottom: 15px; /* Adjust this value for desired spacing */
        }
        /* Reduce spacing for nested lists to keep structure clean */
        ol li ol li {
            margin-bottom: 10px;
        }

    
</style>


	  <title>Robust Reward Modeling Under Noisy Preferences</title>
      <meta property="og:title" content="Robust Reward Modeling Under Noisy Preferences" />
			<meta charset="UTF-8">
  </head>

  <body>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<table class="header" align="left">
								<tr>
									<td colspan="4">
										<span style="font-size: 32px; font-family: 'Courier New', Courier, monospace;">Robust Reward Modeling Under Noisy Preferences</span>
									</td>
								</tr>
								<tr>
										<td align="left">
												<span style="font-size:17px">Iris Xu</span>
										</td>
										<td align="left">
												<span style="font-size:17px">Katherine Liang</span>
										</td>
								</tr>
								<tr>
									<td colspan="4" align="left"><span style="font-size:18px">NeurIPS 2024 (draft)</span></td>
								</tr>
						</table>
					</div>
					<div class="margin-right-block">
					</div>
		</div>

		<div class="content-margin-container" id="outline">
				<div class="margin-left-block">
          <!-- table of contents here -->
          <div style="position:fixed; max-width:inherit; top:max(20%,120px)">
              <b style="font-size:16px">Outline</b><br><br>
              <a href="#introduction">Introduction</a><br><br>
              <a href="#background-and-related-work">Background and Related Work</a><br><br>
              <a href="#methods">Methods</a><br><br>
              <a href="#technical-details">Technical Details</a><br><br>
              <a href="#discussion">Discussion</a><br><br>
              <a href="#conclusion">Conclusion</a><br><br>
          </div>
				</div>
		</div>


    <div class="content-margin-container" id="introduction">
        <div class="margin-left-block">
        </div>
        <div class="main-content-block">
            <h1>Introduction</h1>
<p>Large language models (LLMs) increasingly rely on alignment t...g---whether collected through expert annotators, crowd-sourcing, or even from end users.</p>

<p>Despite the presence of noise, reward models in RLHF are typically trained with standard cross-entropy losses on paired preference data and then used to define the objective for policy learning (e.g., PPO or GRPO).</p>

<p>In contrast, the broader machine-learning literature offers a rich class of robust loss functions designed to handle label and sample noise in classification and regression tasks. These include generalized cross-entropy (GCE), symmetric cross-entropy (SCE), and other robust alternatives to standard cross-entropy, which have demonstrated improvements over cross entropy loss under noisy labels in supervised learning setups.</p>

<p>In RLHF, however, the equivalent of "labels" are human preferences over pairs of model outputs. These preferences are inherently noisy: annotators may disagree on which response is better, introduce systemic bias, or provide inconsistent signal. While policy optimization methods such as Proximal Policy Optimization (PPO) and Group Relative Policy Optimization (GRPO) have been extensively studied, the robustness of the upstream reward models themselves to noisy preferences remains underexplored.</p>

<p>In this work, we investigate techniques from robust classification to improve reward modeling under noisy preference data. Specifically, we focus on the reward-modeling stage of a standard RLHF pipeline and replace the standard Bradley–Terry (BT) cross-entropy loss with noise-robust alternatives such as GCE and SCE. We then evaluate both the standalone reward models and the downstream policies trained with GRPO against a clean, held-out preference test set.</p>

<p>Our main contributions are: ...</p>
        </div>
        <div class="margin-right-block">
        </div>
    </div>


    <div class="content-margin-container" id="background-and-related-work">
        <div class="margin-left-block">
        </div>
        <div class="main-content-block">
            <h1>Background and Related Work</h1>
<p>We briefly review RLHF and preference-based reward modeling, as well as known robust loss functions from the classification literature and prior work on robustness for RLHF-like systems.</p>

<p><b>RLHF and preference modeling.</b>  Modern RLHF pipelines typically proceed in three stages: (1) supervised fine-tuning (SFT) on curated instruction-following data, (2) reward modeling from human preferences over pairs of model responses, and (3) policy optimization against the learned reward model using RL algorithms like PPO or GRPO. The reward model acts as a surrogate for human preferences, guiding the policy toward more aligned behavior.</p>

<p>A common approach to reward modeling uses a Bradley–Terry / logistic preference model over pairs of candidate responses. Given two responses $y^{+}$ and $y^{-}$ to a prompt $x$, a reward model $r_\theta$ assigns scores $r_\theta(y^{+} | x)$ and $r_\theta(y^{-} | x)$, and the probability that annotators prefer $y^{+}$ is modeled as a sigmoid of the reward difference. The reward model is trained with a cross-entropy loss between this predicted preference probability and the observed binary preference label.</p>

<p>Several works have studied the quality, bias, and noise in human feedback used for RLHF-like systems ...5crowdnoise, song2022noisysurvey,eickhoff2018crowdsourcingbias]. These studies highlight that feedback can be inconsistent, corrupted, or systematically biased, which can adversely affect downstream policies trained to optimize such feedback.</p>

<p><b>Robust losses in classification.</b>  In supervised classification with label noise, robust loss functions have been proposed as alternatives to standard cross entropy. Generalized cross-entropy (GCE) interpolates between mean absolute error and cross entropy and has been shown to improve robustness to symmetric and asymmetric label noise. Symmetric cross-entropy (SCE) combines standard cross-entropy with a reverse cross-entropy term, which downweights mislabeled examples by penalizing low-entropy predictions that contradict targets.</p>

<p>Other methods for handling noisy labels include loss correction, sample reweighting, and small-loss selection-based training. These techniques are typically evaluated on vision datasets such as CIFAR-10/100 with synthetic label noise, demonstrating improved robustness over vanilla cross-entropy.</p>

<p><b>Robustness in RLHF and preference learning.</b>  Closer to our setting, prior work on preference-based RL and RLHF has considered adversarial or inconsistent preferences, distribution shift between training and evaluation feedback, and methods for learning under such uncertainty. However, most RLHF pipelines still use standard BT cross-entropy for reward modeling, and robust losses for preference data remain under-explored.</p>

<p>Our work connects these literatures by applying robust classification losses such as GCE and SCE in the reward modeling stage of RLHF and empirically assessing their impact under controlled levels of noisy preferences, both at the reward model level and in downstream GRPO-trained policies.</p>
        </div>
        <div class="margin-right-block">
        </div>
    </div>


    <div class="content-margin-container" id="methods">
        <div class="margin-left-block">
        </div>
        <div class="main-content-block">
            <h1>Methods</h1>
<h2 id="datasets-and-noise-injection">Datasets and Noise Injection</h2>

<p>For training the reward model, we use the UltraFeedback Binarized dataset, which consists of prompts paired with multiple candidate responses and binary preferences indicating which response is better. Each data point can be represented as a triple $(x, y^{+}, y^{-})$, where $x$ is the prompt and $y^{+}, y^{-}$ are the preferred and non-preferred responses, respectively.</p>

<p>To simulate noisy human feedback, we introduce controlled levels of label corruption into the training preferences. Given a clean preference label (e.g., "A is preferred over B"), with probability $p$ we flip the label to its opposite, and with probability $1-p$ we keep the label as is. This corresponds to a simple symmetric label noise model over the binary preference, where $p$ controls the fraction of random preferences. We evaluate noise levels $p\in \{0,0.2,0.4\}$.</p>

<h2 id="reward-modeling-baseline-bradley-terry-loss">Reward Modeling Baseline: Bradley-Terry Loss</h2>

<p>Reward models output a scalar score $r_\theta(y \mid x)$ for each candidate response $y$ given a prompt $x$. We use a transformer-based language model backbone (initialized from a model that has been instruction-tuned / SFT/RLHF) and added a one-layer scalar head to output reward.</p>

<p>Under the standard Bradley--Terry (BT) model, the probability that $y^{+}$ is preferred over $y^{-}$ is:
$$
p_\theta^+ =\sigma\big( r_\theta(y^{+}) - r_\theta(y^{-}) \big), \;\text{where }\sigma(x)=1/(1+e^{-x})
$$
The corresponding loss for a single pair is:
$$
\mathcal{L}_{\mathrm{BT}} = -\log p_\theta^+
$$
for a positive label indicating $y^{+}$ is preferred. With noisy labels, the BT loss is equivalent to binary cross-entropy on pairwise labels.</p>

<h2 id="gce">GCE</h2>

<p>We also consider Generalized Cross-Entropy (GCE) loss, which has been shown to be more robust to noisy labels in classification. For binary classification with predicted probability $p_\theta^+$ for the positive class, the GCE loss with parameter $q\in (0,1]$ is:
$$
\mathcal{L}_{\mathrm{GCE}} = \frac{1-(p_\theta^+)^q}{q}.
$$
When $q \rightarrow 0$, this recovers a log-loss-like behavior, and when $q=1$ it interpolates toward mean absolute error, providing a trade‑off between successfully fitting the data and robustness.</p>

<p>For each pair $(y^+,y^-)$, we first convert the rewards into a probability using the same BT model:
$$
p_\theta^+ = \sigma\big( r_\theta(y^{+}) - r_\theta(y^{-}) \big). $$
We then plug $p_\theta^+$ into the GCE formula for the positive class. For negative labels (if the annotation indicates $y^{-}$ is preferred), we simply swap the roles of $y^{+}$ and $y^{-}$. We treat $q$ as a hyperparameter and sweep over $q \in \{0.3,0.5,0.7,0.9,1.0\}$.</p>

<h2 id="sce">SCE</h2>

<p>Symmetric Cross-Entropy (SCE) combines the standard cross-entropy (CE) with a reverse cross-entropy (RCE) term and has been shown to improve robustness under label noise.</p>

<p>We view the binary preference as a distribution over $\{+,-\}$ with logits given by the reward differences. Let
$$
p\big(+ \mid (y^+, y^-)\big) = p_\theta^{+}, \quad p\big(- \mid (y^+, y^-)\big) = 1 - p_\theta^{+}.
$$

and the one-hot target distribution to be $q$, where
$$
q(+|(y^+,y^-))=1, \quad q(-|(y^+,y^-))=0.
$$

Then, The standard cross-entropy term is
$$\mathrm{CE}(y)=-\sum_{k\in \{+,-\}} q(k|y)\log p(k|y).$$
The reverse cross entropy term swaps the roles of $p$ and $q$:
$$\mathrm{RCE}(y)=-\sum_{k\in \{+,-\}} p(k|y)\log q(k|y),$$
which is implemented in practice by clipping $q$ away from 0 to avoid numerical issues.
The symmetric cross entropy loss for a single pair is then
$$
\mathcal{L}_{\mathrm{SCE}} = \alpha \mathrm{CE} + \beta \mathrm{RCE},
$$
with $\alpha, \beta > 0$ controlling the balance between the standard and reverse terms. Intuitively, the RCE term discourages the model from making over-confident predictions when the target labels are noisy or inconsistent, pushing the predicted distribution away from inconsistent targets.</p>

<p>As with GCE, we use the same reward model architecture as in the BT baseline. We optimize SCE over batches of noisy preference pairs, sweeping over $(\alpha,\beta)$ to identify regimes where $\mathcal{L}_{\mathrm{SCE}}$ is robust to injected noise.</p>

<h2 id="downstream-grpo-evaluation">Downstream GRPO Evaluation</h2>

<p>In addition to evaluating the reward model on a preference test set, we also study the downstream effect on policies trained with Group Relative Policy Optimization (GRPO). For each trained reward model and noise level, we fix the reward model and finetune a policy with GRPO on an instruction-following dataset.</p>

<p>GRPO optimizes a policy to maximize a group-wise relative advantage signal under a KL constraint to the initial SFT policy. In our setup, the reward from the reward model is used as the training objective in GRPO, rather than relying on direct human feedback. At the end of GRPO training, we evaluate the GRPO policy against a strong baseline policy by asking a separate reward model (or human annotators) to compare responses pairwise on a clean held-out evaluation set.</p>

<p>This two-stage setup allows us to determine whether noise-robust reward losses like GCE and SCE not only improve preference prediction under noisy labels but also translate into better downstream policies.</p>

<h2 id="experiment-summary">Experiment Summary</h2>

<p>We summarize our overall experimental setup here. For each noise level $p\in\{0,0.2,0.4\}$, we construct a noisy training set by flipping pairwise preferences with probability $p$, and we keep a clean held‑out validation and test split using pairwise accuracy.</p>

<p>We then use the trained reward models in a two-stage pipeline. First, we evaluate each reward model on the held‑out clean test preferences and report accuracy as a function of noise level and loss function (BT, GCE, SCE). Second, we use the reward model to train a GRPO policy and evaluate the GRPO policy by pairing its responses against a baseline policy and having a separate evaluator model judge which response is preferred. We randomize the order of the responses to avoid positional bias.</p>

<p>This two-stage setup allows us to determine whether noise-robust loss functions produce both better reward models and downstream alignment behavior.</p>
        </div>
        <div class="margin-right-block">
        </div>
    </div>


    <div class="content-margin-container" id="technical-details">
        <div class="margin-left-block">
        </div>
        <div class="main-content-block">
            <h1>Technical Details</h1>
<p><b>Model training.</b> We optimize all reward models with Adam using a learning rate of $10^{-5}$, weight decay, and gradient clipping. We finetune a pretrained transformer model with low-rank adapters (LoRA) on top of a frozen base model; a small dropout of 0.05 was applied on the LoRA branch to regularize the adapters.</p>

<p><b>Hypermparameter Sweep.</b> For GCE and SCE, we perform a grid search over the hyperparameters for each noise level. For GCE, we sweep $q\in\{0.3,0.5,0.7,0.9,1.0\}$. For SCE, we sweep $\alpha\in\{0.1,0.5,0.7,1.5\}$ and $\beta\in\{0.5,0.7,1.0\}$, training each configuration and selecting the best model based on validation accuracy. We repeat this search on each level of noise, and evaluate on the test split of clean preferences.</p>

<p><b>Implementation details.</b> We implement reward model training and GRPO finetuning using open-source libraries for transformer-based language models. All experiments are run on a small cluster of GPUs; training a single reward model takes ... compute-hours depending on the loss function and noise level.</p>
        </div>
        <div class="margin-right-block">
        </div>
    </div>


    <div class="content-margin-container" id="discussion">
        <div class="margin-left-block">
        </div>
        <div class="main-content-block">
            <h1>Discussion</h1>
<h2 id="reward-model-training-results">Reward Model Training Results</h2>

<table border="1" cellpadding="4" cellspacing="0">
<caption>RM accuracy (%) over loss functions and noise levels</caption>
<thead><tr><th>Loss</th><th>Hyperparameters</th><th>Clean</th><th>20% Noise</th><th>40% Noise</th></tr></thead>
<tbody>
<tr><td>BT</td><td>--</td><td><b>75.23</b></td><td><b>73.93</b></td><td><b>67.52</b></td></tr>
<tr><td>GCE</td><td>0.3</td><td>74.59</td><td><b>73.81</b></td><td>66.50</td></tr>
<tr><td></td><td>0.5</td><td>75.04</td><td>73.67</td><td>66.88</td></tr>
<tr><td></td><td>0.7</td><td><b>75.45</b></td><td>73.42</td><td>67.85</td></tr>
<tr><td></td><td>0.9</td><td>72.52</td><td>70.88</td><td><b>69.94</b></td></tr>
<tr><td></td><td>1.0</td><td>72.21</td><td>70.14</td><td>60.33</td></tr>
<tr><td>SCE</td><td>(0.1, 1.0)</td><td>74.84</td><td>71.95</td><td>69.57</td></tr>
<tr><td></td><td>(0.5, 1.0)</td><td><b>75.92</b></td><td>72.54</td><td><b>69.63</b></td></tr>
<tr><td></td><td>(0.7, 0.7)</td><td>74.90</td><td><b>73.48</b></td><td>68.93</td></tr>
<tr><td></td><td>(1.5, 0.5)</td><td>74.65</td><td>72.54</td><td>68.61</td></tr>
</tbody>
</table>

<p>Our standalone reward model experiments demonstrate how robust losses like GCE and SCE behave under varying levels of injected noise. We summarize the main trends considering the best hyperparameters for GCE and SCE below.</p>

<b>Clean data.</b>  When training on clean data ($p=0$), all three losses achieve similar accuracy, with BT and GCE typically slightly outperforming SCE. This indicates that there is little downside to using the standard BT loss on clean data if the hyperparameters are chosen carefully.</p>

<b>Moderate noise.</b>  Under moderate noise ($p=0.2$), the BT baseline remains surprisingly competitive. Its accuracy drops relative to the clean setting, but by a relatively modest amount. GCE and SCE achieve similar or marginally higher accuracy in some configurations, but their gains are not dramatic; in some cases, overly aggressive robustness hyperparameters can over-penalize confident predictions and hurt performance. Overall, the benefits of swapping in a robust loss like GCE or SCE are limited at this noise level.</p>

<b>High noise.</b>  Finally, under the highest noise setting ($p=0.4$), robust losses show clearer benefits. In particular, GCE with intermediate $q$ and SCE with tuned $(\alpha,\beta)$ maintain significantly higher accuracy than BT, which degrades more substantially. This suggests that with enough label corruption, standard cross-entropy starts to overfit noisy labels, while robust losses can partially resist overfitting by down-weighting questionable examples or encouraging higher-entropy predictions that are less likely to latch onto noise.</p>

<p>Overall, these results suggest that robust losses such as GCE and SCE are most beneficial in regimes with substantial preference noise, and their advantages are modest when preferences are relatively clean.</p>

<h2 id="grpo-results">GRPO Results</h2>

<p>We now turn to the downstream GRPO experiments, where we use each trained reward model to finetune a policy and evaluate the resulting policy via pairwise comparisons on a held-out evaluation set. In this evaluation, we compare the GRPO policy trained with each loss against a baseline SFT/RLHF policy, using a separate strong judge model to decide which response is preferred.</p>

<p>Our main findings are:</p>

<ul>
<li>Reward models that perform better on the clean held-out preference test set generally induce better GRPO policies: higher reward accuracy correlates with higher win-rates.</li>
<li>Under clean or low-noise conditions, BT-trained reward models and their corresponding GRPO policies perform competitively with or slightly better than robust-loss-based counterparts.</li>
<li>Under high noise levels, GRPO policies trained on reward models with GCE or SCE show improved win rates over those trained with BT, mirroring the trend observed in reward model accuracy.</li>
</ul>

<p>These trends support the hypothesis that robustness at the reward modeling stage can meaningfully translate into better downstream policies in the presence of significant preference noise.</p>

<h2 id="limitations">Limitations</h2>

<p>Our study has several limitations. First, we inject synthetic symmetric noise into preferences rather than relying on real-world noisy annotator behavior, which may involve more complex, structured noise patterns. Second, we work with a fixed reward model architecture and a single GRPO configuration; different architectures or policy optimization settings may interact with robust losses in non-trivial ways.</p>

<p>Third, we focus on a small set of robust loss functions (GCE and SCE) and do not exhaustively explore alternative approaches such as loss correction, sample reweighting, co-teaching, or more elaborate robust preference models. Finally, our experiments are constrained by compute, limiting the number of runs, hyperparameter sweeps, and ablations we can perform.</p>

<p>These limitations suggest promising directions for future work, including exploring more realistic noise models derived from actual annotator logs, studying robustness across a broader range of model sizes and architectures, and integrating robust reward modeling with robust policy optimization objectives.</p>
        </div>
        <div class="margin-right-block">
        </div>
    </div>


    <div class="content-margin-container" id="conclusion">
        <div class="margin-left-block">
        </div>
        <div class="main-content-block">
            <h1>Conclusion</h1>
<p>We studied the impact of robust loss functions for reward modeling under noisy human preferences in an RLHF-style pipeline. By replacing the standard Bradley--Terry cross-entropy loss with generalized cross-entropy (GCE) and symmetric cross-entropy (SCE), we evaluated both standalone reward model accuracy and downstream GRPO policy performance under varying levels of injected noise.</p>

<p>Our experiments show that:</p>

<ul>
<li>Under clean or low-noise preferences, robust losses behave similarly to the standard BT loss and do not significantly harm reward model or policy performance.</li>
<li>Under higher levels of noise, robust losses can substantially improve reward model accuracy relative to BT, particularly when hyperparameters are tuned appropriately.</li>
<li>These improvements in reward modeling can drive better downstream GRPO policies, as measured by win-rates on held-out pairwise evaluations against a baseline policy.</li>
</ul>

<p>These findings suggest that robust loss functions from the noisy-label classification literature offer a promising, relatively simple way to improve RLHF pipelines in noisy feedback regimes, without modifying the policy optimization algorithm itself. In future work, we aim to extend this analysis to more realistic and structured noise processes, explore combinations with sample selection or reweighting, and study interactions with larger-scale models and more diverse benchmarks.</p>

<p>% Note: this section has been adapted from the NeurIPS style file template. % Please remember that the reference section does not count towards the page limit. % \medskip</p>

<p>% Acknowledgements and broader impact statements can be added here as needed in a camera-ready paper. Use unnumbered first-level heading for % % the references section. The reference section does not count towards the page limit. % \medskip</p>

<p>% { % \small</p>

<p>% \ % [1] Alexander, J.A.\ &amp; Mozer, M.C.\ (1995) Template-based algorithms for connectionist rule extraction. In G. Tesauro, D.S. Touretzky % and T.K. Leen (eds.), {\it Advances in Neural Information Processing Systems 7}, % pp.\ 609--616. Cambridge, MA: MIT Press.</p>

<p>% [2] Bower, J.M.\ &amp; Beeman, D.\ (1995) {\it The Book of GENESIS: Exploring % Realistic Neural Models with the GEneral NEural SImulation System.}  New York: % TELOS/Springer--Verlag.</p>

<p>% [3] Hasselmo, M.E., Schnell, E.\ &amp; Barkai, E.\ (1995) Dynamics of learning and % recall at excitatory recurrent synapses and cholinergic modulation in rat % hippocampal region CA3. {\it Journal of Neuroscience} {\bf 15}(7):5249-5262. % }</p>
        </div>
        <div class="margin-right-block">
        </div>
    </div>


  </body>
</html>