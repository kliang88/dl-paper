<html>
<head>
<script>
    window.MathJax = {
        tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
        }
    };
</script>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<link rel="shortcut icon" href="images/icon.ico">
<style type="text/css">
	body {
		background-color: #f5f9ff;
	}

	/* Hide both math displays initially, will display based on JS detection */
  .mathjax-mobile, .mathml-non-mobile { display: none; }

  /* Show the MathML content by default on non-mobile devices */
  .show-mathml .mathml-non-mobile { display: block; }
  .show-mathjax .mathjax-mobile { display: block; }

	.content-margin-container {
		display: flex;
		width: 100%; /* Ensure the container is full width */
		justify-content: left; /* Horizontally centers the children in the container */
		align-items: center;  /* Vertically centers the children in the container */
	}
	.main-content-block {
		width: 70%; /* Change this percentage as needed */
    max-width: 1100px; /* Optional: Maximum width */
		background-color: #fff;
		border-left: 1px solid #DDD;
		border-right: 1px solid #DDD;
		padding: 8px 8px 8px 8px;
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
	}
	.margin-left-block {
			font-size: 14px;
			width: 15%; /* Change this percentage as needed */
			max-width: 130px; /* Optional: Maximum width */
			position: relative;
			margin-left: 10px;
			text-align: left;
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
			padding: 5px;
	}
	.margin-right-block {
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
			font-size: 14px;
			width: 25%; /* Change this percentage as needed */
			max-width: 256px; /* Optional: Maximum width */
			position: relative;
			text-align: left;
			padding: 10px;  /* Optional: Adds padding inside the caption */
	}

	img {
			max-width: 100%; /* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
	}
	.my-video {
			max-width: 100%; /* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
	}
	/* Hide both video displays initially, will display based on JS detection */
  .vid-mobile, .vid-non-mobile { display: none; }

  /* Show the video content by default on non-mobile devices */
  .show-vid-mobile .vid-mobile { display: block; }
  .show-vid-non-mobile .vid-non-mobile { display: block; }

	a:link,a:visited
	{
		color: #0e7862; /*#1367a7;*/
		text-decoration: none;
	}
	a:hover {
		color: #24b597; /*#208799;*/
	}

	h1 {
		font-size: 18px;
		margin-top: 4px;
		margin-bottom: 10px;
	}

	table.header {
    font-weight: 300;
    font-size: 17px;
    flex-grow: 1;
		width: 70%;
    max-width: calc(100% - 290px); /* Adjust according to the width of .paper-code-tab */
	}
	table td, table td * {
	    vertical-align: middle;
	    position: relative;
	}
	table.paper-code-tab {
	    flex-shrink: 0;
	    margin-left: 8px;
	    margin-top: 8px;
	    padding: 0px 0px 0px 8px;
	    width: 290px;
	    height: 150px;
	}

	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	hr {
    height: 1px; /* Sets the height of the line to 1 pixel */
    border: none; /* Removes the default border */
    background-color: #DDD; /* Sets the line color to black */
  }

	div.hypothesis {
		width: 80%;
		background-color: #EEE;
		border: 1px solid black;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
		font-family: Courier;
		font-size: 18px;
		text-align: center;
		margin: auto;
		padding: 16px 16px 16px 16px;
	}

	div.citation {
    font-size: 0.8em;
    background-color:#fff;
    padding: 10px;
		height: 200px;
  }

	.fade-in-inline {
		position: absolute;
		text-align: center;
		margin: auto;
		-webkit-mask-image: linear-gradient(to right,
																			transparent 0%,
																			transparent 40%,
																			black 50%,
																			black 90%,
																			transparent 100%);
		mask-image: linear-gradient(to right,
																transparent 0%,
																transparent 40%,
																black 50%,
																black 90%,
																transparent 100%);
		-webkit-mask-size: 8000% 100%;
		mask-size: 8000% 100%;
		animation-name: sweepMask;
		animation-duration: 4s;
		animation-iteration-count: infinite;
		animation-timing-function: linear;
		animation-delay: -1s;
	}

	.fade-in2-inline {
			animation-delay: 1s;
	}

	.inline-div {
			position: relative;
	    display: inline-block; /* Makes both the div and paragraph inline-block elements */
	    vertical-align: top; /* Aligns them at the top, you can adjust this to middle, bottom, etc., based on your needs */
	    width: 50px; /* Optional: Adds space between the div and the paragraph */
	}
    .algorithm-container {
		max-width: 600px;
		margin: 0 auto; /* Center the container */
		padding: 20px;
		background-color: #f9f9f9;
		border: 1px solid #ddd;
		border-radius: 5px;
		text-align: left; /* Ensure text within the container is left-aligned */
		display: block; /* Prevent flexbox issues */
	}
	.title {
		font-weight: bold;
		margin-bottom: 10px;
	}
	.code {
		font-family: "Courier New", Courier, monospace;
		background: #f4f4f4;
		padding: 10px;
		border-radius: 5px;
		border: 1px solid #ccc;
		text-align: left; /* Explicitly set text alignment to left */
		display: block;
	}
    .formula {
            font-family: "Courier New", monospace;
            background-color: #f4f4f4;
            padding: 10px;
            border-radius: 5px;
            border: 1px solid #ccc;
            display: inline-block;
        }

    li {
            margin-bottom: 15px; /* Adjust this value for desired spacing */
        }
        /* Reduce spacing for nested lists to keep structure clean */
        ol li ol li {
            margin-bottom: 10px;
        }

    
</style>


	  <title>Robust Reward Modeling Under Noisy Preferences</title>
      <meta property="og:title" content="Robust Reward Modeling Under Noisy Preferences" />
			<meta charset="UTF-8">
  </head>

  <body>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<table class="header" align=left>
								<tr>
									<td colspan=4>
										<span style="font-size: 32px; font-family: 'Courier New', Courier, monospace; /* Adds fallbacks */">Robust Reward Modeling Under Noisy Preferences</span>
									</td>
								</tr>
								<tr>
										<td align=left>
												<span style="font-size:17px"><a href="https://github.com/kliang88">Katherine Liang</a></span>
										</td>
										<td align=left>
												<span style="font-size:17px"><a href="https://github.com/iamxjy">Iris Xu</a></span>
                                    </td>
								<tr>
									<td colspan=4 align=left><span style="font-size:18px">Final project for 6.7960, MIT</span></td>
								</tr>
						</table>
					</div>
					<div class="margin-right-block">
					</div>
		</div>

		<div class="content-margin-container" id="intro">
				<div class="margin-left-block">
          <!-- table of contents here -->
          <div style="position:fixed; max-width:inherit; top:max(20%,120px)">
              <b style="font-size:16px">Outline</b><br><br>
              <a href="#intro">Introduction</a><br><br>
              <a href="#prior_work">Prior Work</a><br><br>
              <a href="#methods_experiments">Methods & Experiments</a><br><br>
              <a href="#implementation">Technical Implementation Details</a><br><br>
              <a href="#discussion">Discussion</a><br><br>
              <a href="#conclusion">Conclusion</a><br><br>
          </div>
				</div>
		</div>

    <div class="content-margin-container" id="intro">
				<div class="margin-left-block">
				</div>
            
		    <div class="main-content-block">
                
                The source code for the work described in this study can be found <a href="https://github.com/iamxjy/dl-rm-loss">here.</a><br><br>
						<h1>Introduction</h1>
                        Large language models (LLMs) increasingly rely on alignment techniques to ensure that their outputs are helpful and consistent with human expectations. A central component of modern alignment pipelines is Reinforcement Learning from Human Feedback (RLHF), where pretrained models are further trained using pairwise human preference data <a href="#ref_1">[1]</a>. In RLHF, preference data is collected through expert annotators, crowd-sourcing, or even from end users via in-product preference prompts. However, this human preference data is frequently noisy: annotators may disagree, lack expertise, follow incorrect biases (e.g. preferring longer responses), or even respond arbitrarily when uncertain, especially if the annotators is not an expert <a href="#ref_2">[2]</a>. This problem has been documented in crowd-sourced labeling studies, showing that such data often contains substantial randomness, inconsistency, and label corruption <a href="#ref_3">[3</a>, <a href="#ref_4">4</a>, <a href="#ref_5">5]</a>. <br><br>
                        
                        Despite the presence of noise, reward models in RLHF are typically trained using the Bradley-Terry (BT) logistic loss, which is an equivalent of binary cross-entropy applied to preference comparisons <a href="#ref_1">[1]</a>. While simple and widely adopted, cross-entropy–based losses are known to be sensitive to noisy labels, leading models to overfit to mislabeled samples and have poor generalization <a href="#ref_6">[6</a>, <a href="#ref_7">7</a>, <a href="#ref_8">8]</a>. This sensitivity is especially problematic for RLHF, where the reward model is almost always further used downstream, for example to define the objective for policy learning (e.g., PPO or GRPO). <br><br>

                        In contrast, the broader machine-learning literature offers a variety of noise-robust loss functions, such as Generalized Cross Entropy (GCE) and Symmetric Cross Entropy (SCE), which have demonstrated improvements over cross entropy loss under noisy settings <a href="#ref_6">[6</a>, <a href="#ref_7">7]</a>.  <br><br>

                        This motivates a natural research question for RLHF: Can we improve preference-based reward modeling, and thereby alignment performance, by replacing the standard BT loss with a noise-robust alternative? In this work, we address this question using controlled noise experiments, training reward models under varying synthetic noise levels with both standard and robust losses, and evaluating their effects on reward accuracy and downstream alignment performance.

		    </div>
		</div>

		<div class="content-margin-container" id="prior_work">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
					<h1>Prior Work</h1>
                    Reinforcement learning from human feedback (RLHF) has become a standard paradigm for aligning large language models (LLMs) with human preferences. Specifically, modern training pipelines typically proceed as follows: a base model is first pre-trained on broad text corpora, then refined through supervised fine-tuning (SFT) on demonstrations of high-quality behavior. Afterward, human labelers compare alternative model outputs, and their preferences are used to train a reward model that predicts how well a given response aligns with human judgments. Finally, a policy optimization reinforcement learning algorithm, such as Proximal Policy Optimization (PPO) or Group Relative Policy Optimization (GRPO), is used to optimize the policy with respect to the learned reward <a href="#ref_9">[9</a>, <a href="#ref_10">10]</a>. <br><br>

                    Within the RLHF pipeline, the reward model (RM) is a critical component. Prior work has shown that stronger reward models directly translate to more aligned policies <a href="#ref_1">[1]</a>. However, much of the existing work on training RMs assumes that the quality of the preference labels used during training is very high <a href="#ref_1">[1</a>, <a href="#ref_11">11]</a>. Specifically, they must be accurate enough that the standard BT loss, known to be sensitive to label noise, is a good choice. However, in practice, real‑world preference datasets may often be, which can distort both the learned reward and the downstream policy. Recent work shows that under synthetic reward signal noise of 10-20\%, standard GRPO can slow dramatically or even collapse <a href="#ref_12">[12]</a>. <br><br>

                    This has motivated a line of work on reward modeling under noisy or varied preferences. InfoRM focuses on reward hacking and misgeneralization, where the reward model focuses on irrelevant features that do not reflect human intent <a href="#ref_13">[13]</a>. They propose an information‑bottleneck objective that filters out irrelevant information in the reward representation. Other approaches model the distribution of label preferences: the Distributional Preference Reward Model (DPRM) models a distribution over crowd preferences for each LLM output and uses optimal‑transport losses to align this predicted distribution with the population-level preference distribution <a href="#ref_14">[14]</a>. Quantile Reward Models (QRMs) learn a quantile‑parameterized distribution over rewards through quantile regression, capturing uncertainty in human feedback <a href="#ref_15">[15]</a>. Broadly, these approaches aim to extract signal from noisy or diverse preferences, rather than treating noise as label corruption inside a classification problem. <br><br>

                    In contrast, we adopt the latter perspective and treat noise as label corruption in a classification setting. Reward modeling can be viewed simply as predicting the preferred response out of two responses for a given prompt. Then, noise arises when annotators make mistakes, click randomly, or follow inconsistent instructions. Framing preference noise as label corruption connects RLHF reward modeling to the broader literature on learning with noisy labels, a much more mature field. <br><br>

                    Specifically, in supervised classification, there have been a few significant works that use different loss functions to train models that are more robust when the data contains noisy labels. Ghosh et al. give theoretical conditions under which loss functions are tolerant to label noise, showing in particular that mean absolute error (MAE) is robust to noise <a href="#ref_16">[16]</a>. <br><br>

                    Although MAE is more robust, it is usually hard to optimize, often underfitting (or converging slowly) when used with modern deep networks <a href="#ref_17">[17]</a>. To address the optimization difficulties of MAE, other loss functions have been proposed to balance robustness and ease of training, such as Generalized Cross Entropy (GCE) loss and Symmetric Cross Entropy (SCE) loss <a href="#ref_6">[6</a>, <a href="#ref_7">7]</a>. These robust losses are simple replacements for standard cross‑entropy and have been shown to significantly improve performance under label noise. Despite this, they have seen limited adoption in RLHF reward modeling. Thus, our study builds on these works by using GCE and SCE loss functions as the objective for reward modeling as opposed to the Bradley-Terry model.

		    </div>
		    <div class="margin-right-block"> <!-- you can move the margin notes up and down with translate -->
		    </div>
		</div>

		<div class="content-margin-container", id="methods_experiments">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
            <h1>Methods and Experiments</h1>
            <h2>Datasets and Noise Injection</h2>
            For training the reward model, we use the UltraFeedback Binarized dataset, a high-quality preference dataset consisting of prompts, pairs of candidate responses, and the corresponding human preference ranking <a href="#ref_18">[18]</a>. For RM training, the binary pairwise comparisons are: $(x,y^+,y^-)$ where $x$ is the prompt, and $y^+$ and $y^-$ are the preferred and non-preferred responses, respectively. <br><br>

            To simulate noisy human feedback, we introduce controlled levels of synthetic label noise into the reward model training dataset. For a given pairwise comparison, we flip the winner with probability $p$, turning each training example into $(x,y^-, y^+)$ with probability $p$, so that $p=0$ is the clean data, and $p=0.5$ would represent fully random preferences. We evaluate noise levels $p\in \{0,0.2,0.4\}$. <br><br>

            <h2>Reward Modeling Baseline: Bradley-Terry Loss</h2>
            Reward models output a scalar score $r_\theta(y \mid x)$ for each candidate response to represent how good response $y$ is for a given prompt $x$. Thus, for each reward model we implemented, we use a base model (Qwen3-0.6B-Base, which is pretrained but has not undergone any posttraining, e.g. SFT/RLHF) and added a one-layer scalar head to output reward. <br><br>

            Under the standard Bradley--Terry (BT) model, the probability that $y^{+}$ is preferred over $y^{-}$ is: $$p_\theta^+ =\sigma\big( r_\theta(y^{+}) - r_\theta(y^{-}) \big), \;\text{where}\sigma(x)=1/(1+e^{-x}).$$ The corresponding loss for a single pair is: $$\mathcal{L}_{\mathrm{BT}} = -\log p_\theta^+ = -\log \sigma\big( r_\theta(y^{+}) - r_\theta(y^{-}) \big).$$ This objective is equivalent to binary cross-entropy on pairwise labels. <br><br>

            <h2>Generalized Cross-Entropy (GCE)</h2>
            We also consider Generalized Cross-Entropy (GCE) loss, which interpolates between standard cross‑entropy and mean absolute error using a parameter $q\in (0, 1]$ <a href="#ref_6">[6]</a>. Thus, this achieves a better trade‑off between successfully fitting the data and robustness. <br><br>

            For each pair $(y^+,y^-)$, we first convert the rewards into the probability that $y^+$ is preferred over $y^-$: $$p_\theta^+ = \sigma\big( r_\theta(y^{+}) - r_\theta(y^{-}) \big).$$ <br><br>

            Treating the preferred response $y^+$ as the positive class, the GCE loss for a single pair is then: $$\mathcal{L}_{\mathrm{GCE}} = \frac{1-(p_\theta^+)^q}{q}.$$ <br><br>

            <h2>Symmetric Cross-Entropy (SCE)</h2>
            In addition to GCE, we also test Symmetric Cross-Entropy (SCE), which augments the standard cross‑entropy with a reverse cross‑entropy term <a href="#ref_7">[7]</a>. Similarly, this loss function improves performance on noisy labels, under both symmetric and asymmetric noise settings. <br><br>

            Given the same probabilities from the rewards: $$p_\theta^+ = P(y^{+} \succ y^{-}) = \sigma\big( r_\theta(y^{+}) - r_\theta(y^{-}) \big).$$ and the one-hot target distribution to be $q$, where $$q(+|(y^+,y^-))=1, \quad q(-|(y^+,y^-))=0.$$ <br><br>

            Then, the standard cross-entropy term is $$\mathrm{CE}(y)=-\sum_{k\in \{+,-\}} q(k|y)\log p(k|y).$$ The reverse cross entropy term swaps the roles of $p$ and $q$: $$\mathrm{RCE}(y)=-\sum_{k\in \{+,-\}} p(k|y)\log q(k|y),$$ which is implemented in practice by clipping $q$ away from 0 to avoid numerical issues. The symmetric cross entropy loss for a single pair is then $$\mathcal{L}_{\mathrm{SCE}} = \alpha \mathrm{CE} + \beta \mathrm{RCE},$$ with $\alpha, \beta > 0$ controlling the balance between the standard and reverse terms. Intuitively, the CE term encourages the model to place high probability on the preferred response, while the RCE term penalizes over‑confident assignments to the potentially noisy label by pushing the predicted distribution away from inconsistent targets. <br><br>

            As with GCE, we use the same reward model architecture as in the BT baseline and swap the loss for $\mathcal{L}_{\mathrm{SCE}}$, with $\alpha$ and $\beta$ as hyperparameters. <br><br>

            <h2>Downstream GRPO Evaluation</h2>
            In addition to evaluating the reward model on a preference test set, we also evaluate whether improved reward model robustness translates to improved policy learning. So, we fine-tune two Qwen3‑0.6B‑Base policies on UltraChat <a href="#ref_19">[19]</a>: one using the reward model for 40\% noise trained with BT loss, and one using the best reward model variant for 40\% noise (which ends up being GCE with $q=0.9$). We then compare the two policies using LM-as-a-judge on 1,024 prompts from a held‑out split of UltraChat and on the Arena-Hard-Auto dataset <a href="#ref_20">[20]</a>. Arena-Hard-Auto contains difficult user queries sourced from Chatbot Arena: 500 challenging real‑world questions (e.g., software engineering problems, math questions, etc.) and 250 creative‑writing prompts. For each prompt from these datasets, we query GPT-5.1 to choose which of the two policy responses is better (being careful to randomize the order of the responses to avoid positional bias. <br><br>

            This two-stage setup allows us to determine whether noise-robust losses produce both better reward models and downstream alignment behavior. <br><br>

            <h2>Experiment Summary</h2>
            We summarize our overall experimental setup here. For each noise level $p \in \{0, 0.2, 0.4\}$, we add noise to the UltraFeedback preference training data with probability $p$, and train a reward model on the resulting dataset. All reward models share the same pre-trained base model, Qwen3‑0.6B‑Base with a single scalar head, and differ only in the training objective: BT loss, GCE, or SCE. For each configuration of loss and noise level, we train the reward model on the noisy training split and evaluate it on a clean held‑out validation and test split using pairwise accuracy. <br><br>

            We then use the trained reward models in a two-stage pipeline. First, we evaluate their standalone accuracy on the UltraFeedback test set. Second, for the 40\% noise level, we take the baseline BT reward model and the best robust variant (GCE with $q=0.9$) and use each as the learned reward to fine-tune a Qwen3-0.6B-Base policy with GRPO. The resulting downstream policies are compared using LM-as-a-judge, with GPT 5.1 serving as the judge model, and we report the corresponding win rates. <br><br>

		    </div>
		</div>

		<div class="content-margin-container" id="implementation">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>Technical Implementation Details</h1>
						All experiments in this study were conducted using an Nvidia A40 GPU. <br><br>

                        As for the training details, we used an Adam optimizer with an initial learning rate of \(10^{-4}\). A cosine-annealing learning rate scheduler was also used with a minimum learning rate of \(10^{-5}\) for a maximum number of steps equal to the number of training epochs. We also implement early stopping for our runs, with a patience of 5 epochs, tracking the validation accuracy for any improvements each epoch. <br><br>

                        <div class="algorithm-container">
                            <div class="title">Algorithm: Early Stopping Algorithm</div>
                            <div><strong>Inputs:</strong> Validation Metric (<code>CurrentValue</code>), Patience (<code>P</code>), Minimum Delta (<code>&delta;</code>)</div>
                            <div><strong>Initialization:</strong> <code>BestValue &larr; -∞</code>, <code>Counter &larr; 0</code></div>
                            <div class="code">
                                While Training is ongoing:<br>
                                &nbsp;&nbsp;If (<code>CurrentValue &gt; BestValue + δ</code>):<br>
                                &nbsp;&nbsp;&nbsp;&nbsp;BestValue &larr; CurrentValue<br>
                                &nbsp;&nbsp;&nbsp;&nbsp;Counter &larr; 0<br>
                                &nbsp;&nbsp;Else:<br>
                                &nbsp;&nbsp;&nbsp;&nbsp;Counter &larr; Counter + 1<br>
                                &nbsp;&nbsp;If (<code>Counter &geq; P</code>):<br>
                                &nbsp;&nbsp;&nbsp;&nbsp;Stop Training<br>
                                &nbsp;&nbsp;&nbsp;&nbsp;Break
                            </div>
                        </div><br><br>

                        Finally, we set a seed for all runs to ensure that all runs are consistent and not due to random chance. For the sake of reducing computation time, we subsetted the Food101 dataset to take 50% of the all training and validation samples. All datasets were trained and validated using batch sizes of 128 and 512 images, respectively. 


		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

        <div class="content-margin-container" id="discussion">
            <div class="margin-left-block">
            </div>
        <div class="main-content-block">
                    <h1>Discussion</h1>
                    Now, let's interpret the results of our study.<br><br>

                    <i>Freezing encoder layers during training:</i><br><br>

					<img src="./figures/fl_no_freeze_all_freeze.png" width="600px" style="padding-top: 50px;"/>
					<img src="./figures/dtd_no_freeze_all_freeze.png" width="600px" style="padding-bottom: 50px;"/>

                    Freezing the transformer encoder layers during fine-tuning proved to be detrimental to the models’ accuracy and convergence time. As shown in Figure 3, in which the models were trained following the vanilla training scheme, the models with frozen transformer encoders took many more epochs/batches to converge compared to those without any freezing. Furthermore, the frozen model converged at a significantly lower accuracy value (66.3%) than the unfrozen model (73.6%) on DTD, as shown in Figure 3.<br><br>

                    <i>ADS for Transfer Learning:</i><br><br>
                    Several combinations of ADS training hyperparameters achieved higher accuracy and quicker convergence than vanilla training on the Oxford Flowers and DTD datasets. Note that we use the number of batches required for convergence rather than the number of epochs for comparing different training schemes since ADS uses much fewer batches in the reweighted epochs.<br><br>

                    <img src="./figures/fl_mixone_nomixone.png" width="600px" style="padding-top: 50px;"/>

                    First, the alternating phases approach proved much more successful than using only targeted epochs, as shown in Figure 4. Shifting to only the worst-performing classes is prone to overfitting; the model begins to “forget” about the classes it performed well on. Therefore, we used only the alternating phases approach in subsequent experiments. <br><br>
					
					<div style="text-align: center; margin-bottom: 5px;">
						<b>Flowers</b>
					</div>
					<table border="1" cellpadding="8" cellspacing="0">
						<thead>
							<tr>
								<th>Model</th>
								<th>Training Scheme</th>
								<th>Targeted Epoch Frequency</th>
								<th># of Top-k Bad Classes</th>
								<th>Highest Accuracy*</th>
								<th># batches for convergence</th>
							</tr>
						</thead>
						<tbody>
							<tr>
								<td>1</td>
								<td>Vanilla</td>
								<td>N/A</td>
								<td>N/A</td>
								<td>97.71%</td>
								<td>440</td>
							</tr>
							<tr>
								<td>2</td>
								<td>Ours</td>
								<td>2</td>
								<td>25</td>
								<td><b style="color:blue">97.84%</b></td>
								<td>393</td>
							</tr>
							<tr>
								<td>3</td>
								<td>Ours</td>
								<td>4</td>
								<td>25</td>
								<td>97.74%</td>
								<td><b style="color:blue">305</b></td>
							</tr>
						</tbody>
					</table><br><br>

                    
                    As shown in Table 1, taking the 25 classes with the lowest validation accuracy in each reweighting phase achieved slightly higher accuracies after training with significantly fewer batches. Model 2, trained by inserting targeted epochs every 2 epochs, achieved ~97.7% accuracy in a similar number of batches to Model 3, but was able to continue learning before converging at 97.8% accuracy in significantly fewer batches than the vanilla approach.

					<div style="text-align: center; margin-bottom: 5px;">
						<b>DTD</b>
					</div>
					<table border="1" cellpadding="8" cellspacing="0">
						<thead>
							<tr>
								<th>Model</th>
								<th>Training Scheme</th>
								<th>Targeted Epoch Frequency</th>
								<th># of Top-k Bad Classes</th>
								<th>Highest Accuracy*</th>
								<th># batches for convergence</th>
							</tr>
						</thead>
						<tbody>
							<tr>
								<td>1</td>
								<td>Vanilla</td>
								<td>N/A</td>
								<td>N/A</td>
								<td>73.55%</td>
								<td>255</td>
							</tr>
							<tr>
								<td>2</td>
								<td>Ours</td>
								<td>2</td>
								<td>5</td>
								<td>73.59</td>
								<td><b style="color:blue">225</b></td>
							</tr>
							<tr>
								<td>3</td>
								<td>Ours</td>
								<td>3</td>
								<td>12</td>
								<td><b style="color:blue">73.92%</b></td>
								<td>183</td>
							</tr>
						</tbody>
					</table><br><br>

                    Experiments run on DTD achieved similar results to those run on Oxford Flowers, as shown in Table 2. Model 2 converged to a similar accuracy to Model 1 (vanilla), but in significantly fewer batches. Furthermore, Model 3 achieved significantly higher accuracy in more batches than Model 2, but still fewer batches than the vanilla approach. <br><br>
                    
					<img src="./figures/dtd_prop_comparison.png" width="600px" style="padding-top: 50px;"/>

                    Changing the proportion of weight in the reweighted dataset given to the top-k bad classes was inconsequential; as shown in Figure 5, all models achieved very similar training histories.<br><br>

                    <i>Training from Scratch on Food101:</i><br><br>

                    <img src="./figures/food_from_scratch.png" width="600px" style="padding-top: 50px;"/>

                    ADS was much more successful than vanilla training when training a ViT from scratch. As shown in Figure 6, vanilla training converged at ~16% accuracy on the Food101 dataset, while ADS, despite its intrinsically more unstable training history due to the insertion of reweighted epochs, continued to grow past ~34% accuracy (training stopped early due to computational constraints). We believe that this shows great promise in proving that ADS allows models to continue learning valuable information in later epochs due to the presence of targeted epochs, although further experimentation on training models from scratch is needed to prove that this is the case.


                    



        </div>
        <div class="margin-right-block" style="transform: translate(0%, -100%);">
			<p style="transform: translate(0%, -300%);">
				<b>Figure 3.</b> Comparing parameter freezing with no parameter freezing for fine-tuning on Oxford Flowers (top) and DTD (bottom).
			</p>

			<p style="transform: translate(0%, 500%);">
				<b>Figure 4.</b> Results of comparing the alternating phases scheme with the only targeted epochs scheme on Oxford Flowers. Note that alternating phases converges to a significantly higher validation accuracy.
			</p>
		
			<p style="transform: translate(0%, 700%);">
				<b>Table 1.</b> Results of vanilla and ADS training with several hyperparameters on Oxford Flowers. All of our runs were run with alternating phases.

				*the accuracy described here is the average accuracy over a sliding window of width 3.
			</p>
		
			<p style="transform: translate(0%, 900%);">
				<b>Table 2.</b> Results of vanilla and ADS training with several hyperparameters on DTD. All of our runs were run with alternating phases.
				
				*the accuracy described here is the average accuracy over a sliding window of width 3.
			</p>

			<p style="transform: translate(0%, 2300%);">
				<b>Figure 5.</b> Results of varying the proportion of weight given to the worst classes in the reweighted dataset on DTD.
			</p>

			<p style="transform: translate(0%, 3200%);">
				<b>Figure 6.</b> Results of training ViTs from scratch, once with vanilla training and once with our ADS scheme.
			</p>
		</div>


    </div>

    <div class="content-margin-container" id="conclusion">
        <div class="margin-left-block">
        </div>
    <div class="main-content-block">
                <h1>Conclusion</h1>
                In this study, we investigated a new training paradigm for optimizing image classification models called Adaptive Data Selection (ADS). Whereas typical optimization methods involve running an optimizer on randomly selected or predetermined batches of training data (e.g. curriculum learning), ADS utilizes a dynamic training selection procedure that tailors training toward the model's specific weaknesses, focusing on underperforming regions of the training data distribution. Specifically, in a classification problem, our ADS framework evaluates the model's performance after some number of training epochs and identifies the top-k classes where the model is performing the worst in (in terms of classification accuracy). Then, some subsequent training epochs prioritize data from these worst performing classes, allowing the model to improve on its deficiencies. The rationale behind this is that, in order to speed up convergence with limited data, ADS can effectively concentrate its computational resources on resolving performance bottlenecks. This targeted process of identifying poor performing classes and training on them is repeated until convergence.<br><br>
                Our experiments explored the effectiveness of ADS in various scenarios, including transfer learning with pre-trained ViT models and training ViTs up from scratch. Our findings on the applicability of ADS to transfer learning include:

                <ol>
                    <li>
                        ADS showed faster convergence and outperformed standard stochastic training methods. In the Oxford Flowers dataset, ADS achieved higher accuracy with much fewer training batches than traditional methods. A model trained with ADS, with an accuracy of 97.74%, outperformed baseline models while using fewer batches (305 vs 440).
                    </li>
                    <li>
                        On the Describable Textures Dataset (DTD), ADS similarly showed notable improvements in convergence rates (183 batches vs 255 batches) while exceeding baseline accuracy, further validating its effectiveness across different datasets.
                    </li>
                    <li>
                        We also ran transfer learning experiments where we trained one model while freezing the transformer encoder layers and one model while not freezing any weights. The motivation for freezing the transformer weights was that, with less tunable parameters, the model would need to learn more efficiently and thus lead to faster convergence in terms of total data points trained on and total epochs. However, we found that this freezing technique resulted in markedly lower performance and took many more epochs and batches to converge when fine-tuned on the Flowers dataset. Additionally, the frozen model achieved a substantially lower validation accuracy of 66.3% than the free unfrozen model with a validation accuracy of 73.6% when trained on the DTD dataset. This leads us to believe that freezing the transformer encoder may not allow enough model expressivity.
                    </li>
                </ol> 


                We also trained models from scratch using ADS. In particular, we investigated the performance of ADS by training a classification model on the Food101 dataset. We trained two vision transformers of the same dimensions: one using the targeted ADS data selection method and one using stochastic data selection. We found that the ViT trained using ADS (validation accuracy 0.336) significantly outperforms the standard ViT (validation accuracy 0.163). It is worth noting that due to our limited model size, training capabilities, and computational resources, our models are not performing as well as state-of-the-art models. However, the relative performance between the two models under these constraints is clear. <br><br>

                ADS represents a step toward efficient and adaptive methods of training in deep learning. In this regard, it is especially promising for applications where either the training data is severely imbalanced or the available computational power is limited. An extension to reinforcement learning would aim at prioritizing states and actions where policies are not performing well, thus leading to faster convergence in complex environments. In natural language processing, ADS may dynamically focus on ambiguous samples, which could enhance model performance for tasks such as sentiment analysis or machine translation. Future development of the ADS algorithm could focus on optimizing how many top-k classes to choose for training or by performing hyperparameter sweeps on the number of epochs per training section in our AP or TOTE training schemes. It is worth nothing that in our experiments, we were limited by our computing power and were unable to run extensive trials to exhaustively test our conclusions. Thus, to make the method more solid and proven, more trials would need to be conducted to ensure that the difference in performance is statistically significant. Additionally, the choice of datasets and hyperparameters were also partially chosen to ensure that we would have time to run all experiments that we deemed necessary. This may have led to unoptimal parameter choices for select a select few experiments and may require further study.






                


    </div>
    <div class="margin-right-block">
    </div>
</div>

		<div class="content-margin-container" id="citations">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<div class='citation' id="references" style="height:auto"><br>
							<span style="font-size:16px">References:</span><br><br>
							<a id="ref_1"></a>[1] <a href="https://arxiv.org/abs/2010.11929">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a>, Dosovitskiy et al., 2020<br><br>
							<a id="ref_2"></a>[2] <a href="https://arxiv.org/pdf/2405.14813">Scalable Optimization in the Modular Norm</a>, Large et al., 2024<br><br>
							<a id="ref_3"></a>[3] <a href="https://arxiv.org/abs/1412.6980">Adam: A Method for Stochastic Optimization</a>, Kingma et al., 2014<br><br>
							<a id="ref_4"></a>[4] <a href="https://arxiv.org/abs/2208.09632">Adam Can Converge Without Any Modification On Update Rules</a>, Zhang et al., 2022<br><br>
							<a id="ref_5"></a>[5] <a href="https://arxiv.org/abs/2311.05589">A Coefficient Makes SVRG Effective</a>, Yin et al., 2023<br><br>
							<a id="ref_6"></a>[6] <a href="https://www.sciencedirect.com/science/article/pii/0010027793900584">Learning and development in neural networks: the importance of starting small</a>, Elman, 1993<br><br>
							<a id="ref_7"></a>[7] <a href="https://arxiv.org/abs/1808.01097">CurriculumNet: Weakly Supervised Learning from Large-Scale Web Images</a>, Guo et al., 2018<br><br>
							<a id="ref_8"></a>[8] <a href="https://arxiv.org/abs/2010.03255">Variational Feature Disentangling for Fine-Grained Few-Shot Classification</a>, Xu et al., 201<br><br>
						</div>
		    </div>
		    <div class="margin-right-block">
            <!-- margin notes for reference block here -->
		    </div>
		</div>

	</body>

</html>