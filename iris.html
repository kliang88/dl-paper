<html>
<head>
<script>
	window.MathJax = {
		tex: {
		inlineMath: [['$', '$'], ['\\(', '\\)']],
		displayMath: [['$$', '$$'], ['\\[', '\\]']]
		}
	};
</script>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<link rel="shortcut icon" href="images/icon.ico">
<style type="text/css">
	body {
		background-color: #f5f9ff;
	}

	/* Hide both math displays initially, will display based on JS detection */
  .mathjax-mobile, .mathml-non-mobile { display: none; }

  /* Show the MathML content by default on non-mobile devices */
  .show-mathml .mathml-non-mobile { display: block; }
  .show-mathjax .mathjax-mobile { display: block; }

	.content-margin-container {
		display: flex;
		width: 100%; /* Ensure the container is full width */
		justify-content: left; /* Horizontally centers the children in the container */
		align-items: center;  /* Vertically centers the children in the container */
	}
	.main-content-block {
		width: 70%; /* Change this percentage as needed */
    max-width: 1100px; /* Optional: Maximum width */
		background-color: #fff;
		border-left: 1px solid #DDD;
		border-right: 1px solid #DDD;
		padding: 8px 8px 8px 8px;
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
	}
	.margin-left-block {
			font-size: 14px;
			width: 15%; /* Change this percentage as needed */
			max-width: 130px; /* Optional: Maximum width */
			position: relative;
			margin-left: 10px;
			text-align: left;
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
			padding: 5px;
	}
	.margin-right-block {
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
			font-size: 14px;
			width: 25%; /* Change this percentage as needed */
			max-width: 256px; /* Optional: Maximum width */
			position: relative;
			text-align: left;
			padding: 10px;  /* Optional: Adds padding inside the caption */
	}

	img {
			max-width: 100%; /* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
	}
	.my-video {
			max-width: 100%; /* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
	}
	/* Hide both video displays initially, will display based on JS detection */
  .vid-mobile, .vid-non-mobile { display: none; }

  /* Show the video content by default on non-mobile devices */
  .show-vid-mobile .vid-mobile { display: block; }
  .show-vid-non-mobile .vid-non-mobile { display: block; }

	a:link,a:visited
	{
		color: #0e7862; /*#1367a7;*/
		text-decoration: none;
	}
	a:hover {
		color: #24b597; /*#208799;*/
	}

	h1 {
		font-size: 18px;
		margin-top: 4px;
		margin-bottom: 10px;
	}

	table.header {
    font-weight: 300;
    font-size: 17px;
    flex-grow: 1;
		width: 70%;
    max-width: calc(100% - 290px); /* Adjust according to the width of .paper-code-tab */
	}
	table td, table td * {
	    vertical-align: middle;
	    position: relative;
	}
	table.paper-code-tab {
	    flex-shrink: 0;
	    margin-left: 8px;
	    margin-top: 8px;
	    padding: 0px 0px 0px 8px;
	    width: 290px;
	    height: 150px;
	}

	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	hr {
    height: 1px; /* Sets the height of the line to 1 pixel */
    border: none; /* Removes the default border */
    background-color: #DDD; /* Sets the line color to black */
  }

	div.hypothesis {
		width: 80%;
		background-color: #EEE;
		border: 1px solid black;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
		font-family: Courier;
		font-size: 18px;
		text-align: center;
		margin: auto;
		padding: 16px 16px 16px 16px;
	}

	div.citation {
    font-size: 0.8em;
    background-color:#fff;
    padding: 10px;
		height: 200px;
  }

	.fade-in-inline {
		position: absolute;
		text-align: center;
		margin: auto;
		-webkit-mask-image: linear-gradient(to right,
																			transparent 0%,
																			transparent 40%,
																			black 50%,
																			black 90%,
																			transparent 100%);
		mask-image: linear-gradient(to right,
																transparent 0%,
																transparent 40%,
																black 50%,
																black 90%,
																transparent 100%);
		-webkit-mask-size: 8000% 100%;
		mask-size: 8000% 100%;
		animation-name: sweepMask;
		animation-duration: 4s;
		animation-iteration-count: infinite;
		animation-timing-function: linear;
		animation-delay: -1s;
	}

	.fade-in2-inline {
			animation-delay: 1s;
	}

	.inline-div {
			position: relative;
	    display: inline-block; /* Makes both the div and paragraph inline-block elements */
	    vertical-align: top; /* Aligns them at the top, you can adjust this to middle, bottom, etc., based on your needs */
	    width: 50px; /* Optional: Adds space between the div and the paragraph */
	}
    .algorithm-container {
		max-width: 600px;
		margin: 0 auto; /* Center the container */
		padding: 20px;
		background-color: #f9f9f9;
		border: 1px solid #ddd;
		border-radius: 5px;
		text-align: left; /* Ensure text within the container is left-aligned */
		display: block; /* Prevent flexbox issues */
	}
	.title {
		font-weight: bold;
		margin-bottom: 10px;
	}
	.code {
		font-family: "Courier New", Courier, monospace;
		background: #f4f4f4;
		padding: 10px;
		border-radius: 5px;
		border: 1px solid #ccc;
		text-align: left; /* Explicitly set text alignment to left */
		display: block;
	}
    .formula {
            font-family: "Courier New", monospace;
            background-color: #f4f4f4;
            padding: 10px;
            border-radius: 5px;
            border: 1px solid #ccc;
            display: inline-block;
        }

    li {
            margin-bottom: 15px; /* Adjust this value for desired spacing */
        }
        /* Reduce spacing for nested lists to keep structure clean */
        ol li ol li {
            margin-bottom: 10px;
        }

    
</style>


	  <title>Adaptive Data Selection (ADS) Learning for Image Classification</title>
      <meta property="og:title" content="Adaptive Data Selection (ADS) Learning for Image Classification" />
			<meta charset="UTF-8">
  </head>

  <body>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<table class="header" align=left>
								<tr>
									<td colspan=4>
										<span style="font-size: 32px; font-family: 'Courier New', Courier, monospace; /* Adds fallbacks */">Adaptive Data Selection (ADS) Learning for Image Classification: A Dive into Transfer Learning Dynamics</span>
									</td>
								</tr>
								<tr>
										<td align=left>
												<span style="font-size:17px"><a href="https://github.com/kevinchengg">Kevin Cheng</a></span>
										</td>
										<td align=left>
												<span style="font-size:17px"><a href="https://github.com/mfang92">Mason Fang</a></span>
										</td>
                                        <td align=left>
                                            <span style="font-size:17px"><a href="https://github.com/ofoofoo">Orion Foo<Footer></Footer></a></span>
                                    </td>
								<tr>
									<td colspan=4 align=left><span style="font-size:18px">Final project for 6.7960, MIT</span></td>
								</tr>
						</table>
					</div>
					<div class="margin-right-block">
					</div>
		</div>

		<div class="content-margin-container" id="intro">
				<div class="margin-left-block">
          <!-- table of contents here -->
          <div style="position:fixed; max-width:inherit; top:max(20%,120px)">
              <b style="font-size:16px">Outline</b><br><br>
              <a href="#intro">Introduction</a><br><br>
              <a href="#prior_work">Prior Work</a><br><br>
              <a href="#methods_experiments">Methods & Experiments</a><br><br>
              <a href="#implementation">Technical Implementation Details</a><br><br>
              <a href="#discussion">Discussion</a><br><br>
              <a href="#conclusion">Conclusion</a><br><br>
          </div>
				</div>
		</div>

    <div class="content-margin-container" id="intro">
				<div class="margin-left-block">
				</div>
            
		    <div class="main-content-block">
                
                The source code for the work described in this study can be found <a href="https://github.com/ofoofoo/effective_transfer_learning">here.</a><br><br>
						<h1>Introduction</h1>
                        In the field of computer vision, the ability of models to accurately classify images has long been a sought after and important property. Transfer learning has emerged as a powerful paradigm in machine learning, enabling the use of models pre-trainedon larger datasets to help serve as a starting point for training our model, especially in the case of limited training data. By leveraging knowledge learned from large-scale datasets, transfer learning promises to significantly reduce the computational and data requirements for developing accurate models in diverse domains. <br><br>
                        Image classification had been dominated by convolutional neural networks (CNNs) prior to the last 4 years. However, the recent introduction of the vision transformer (ViT) architecture has offered a novel and more accurate model for processing visual data <a href="#ref_1">[1]</a>. ViTs divide images into fixed-size patches which are flattened, linearly embedded, and appended with positional encodings to preserve spatial relationships. These embeddings are then processed through a transformer encoder which uses multi-head self-attention to capture global relationships between patches. Finally, this encoding is run through an MLP head, which outputs a classification. A diagram summarizing the flow of a vision transformer can be found below:<br><br>
                        
                        <img src="./figures/vit.png" width=600px/>

                        Unlike CNNs, ViTs do not inherently prioritize locality or translation invariance, which allows them to excel at capturing long-range dependencies. Thus, by leveraging self-attention mechanisms, ViTs can capture global relationships within an image more effectively than the local receptive fields of CNNs, resulting in learned features that can be used for tasks including object detection, segmentation, and fine-grained classification. <br><br>
                        In our study, we aim to investigate whether a ViT trained on a large image dataset can serve as an efficient starting model from which we can fine-tune for smaller, more specific image datasets. Additionally, rather than regular batch gradient descent, we introduce a new model training paradigm that emphasizes training on the classes that have the lowest validation accuracies after some number of epochs. <br><br>

                        In contrast to traditional optimization methods that typically rely on randomly sampled batches of training data to update model parameters, our new method, Adaptive Data Selection (ADS), takes a more focused approach by targeting the worst-performing classes during training. The key innovation of ADS is its adaptive selection of training data based on model performance. For instance, in a classification task with 100 classes, after training the model for a few epochs, we can conduct a validation phase to identify the 25 classes where the model is performing the worst. Instead of continuing with random data batches or balanced class distributions, ADS prioritizes data from these poorly performing classes and trains the model further on them for a few epochs. This process is repeated, refining the model's understanding by consistently exposing it to the most challenging data points, until convergence is achieved. <br><br>
                        In the following sections, we investigate the performance of ADS when used as an optimization technique for faster convergence than known methods, as well as its behavior in the context of transfer learning. <br><br>

		    </div>
            <div class="margin-right-block" style="transform: translate(0%, -20%);"> <!-- you can move the margin notes up and down with translate -->
                <b>Figure 1.</b> From <a href="https://arxiv.org/abs/2010.11929">Dosovitskiy et al. (2021) </a>: <i>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</i>. A ViT splits an image into patches, encodes them with positional embeddings, processes them through a transformer encoder, and uses a classification head to predict image classes.
                  </div>
		</div>

		<div class="content-margin-container" id="prior_work">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
					<h1>Prior Work</h1>
                    Recently, there has been significant exploration of alternative optimization methods for training classification models beyond traditional stochastic gradient descent (SGD). These efforts aim to improve convergence rates, robustness, and adaptability to diverse tasks. In particular, steepest descent, Adam, and Stochastic Variance Reduced Gradient (SVRG) have emerged as prominent methods, each trained on large datasets. <br><br>

                    Steepest descent optimization is a gradient-based method designed to minimize a modified loss function by iteratively selecting the largest modified gradient descent direction. As opposed to conventional gradient descent, steepest descent introduces additional structure, often done by modifying the loss function or using weighted norms to determine the steepest decrease direction <a href="#ref_2">[2]</a>. For example, the steepest descent technique can adaptively scale the gradient with a preconditioner matrix or modify step lengths to ensure optimal decrease at each iteration. This, in turn, may lead to faster convergence in certain problems where the curvature of the loss function is complex, as it adapts the direction of descent more dynamically to align better with the optimal path in the geometry of the function.<br><br>
                    Another popular modification to stochastic gradient descent is Adam. Adam is a robust optimization technique that builds upon both RMSprop and momentum. It keeps running averages for both the gradients and the squared magnitudes of the gradients, thus adaptively changing the learning rate for each parameter. This is very useful in those models where gradients are sparse or noisy. By incorporating adaptive learning rates into momentum, Adam can achieve faster convergence with less oscillations. It has seen extensive usage in deep learning tasks due to its efficiency in high-dimensional parameter spaces and general reliability across various architectures (<a href="#ref_3">[3]</a>, <a href="#ref_4">[4]</a>). <br><br>

                    In addition to Adam, Stochastic Variance Reduced Gradient (SVRG) is also a technique utilized for more stable convergence. SVRG overcomes one of the major limitations of stochastic gradient descent: the variance in the gradient estimates that may lead to slow convergence. By periodically computing a full gradient on the entire dataset and using it to correct the stochastic gradient updates, SVRG reduces the variance and stabilizes convergence. This approach works particularly well for convex and strongly convex loss functions and has shown faster convergence rates compared to traditional SGD <a href="#ref_5">[5]</a>. SVRG strikes a balance between computational overheads and faster iteration, hence almost always practically applicable for large-scale machine learning problems.<br><br>

                    While steepest descent, Adam, and SVRG provide optimizers that lead to more stability and faster convergence, the data they are trained on is stochastically chosen from the whole training set and act as a representation of the entire training dataset. Instead of modifying the optimizer, Curriculum Learning (CL) supervises and progressively changes the data that the model is being trained on <a href="#ref_6">[6]</a>. CL is a machine learning technique that takes inspiration from how humans learn things: first the easy concepts, then advanced. The idea behind it is to organize the training data in an easy-to-hard manner, leading the model through progressively harder tasks. The key advantage of CL is that it first lets the model learn the “concept” before diving into more challenging problems in order to enable efficient and stable training. This method contrasts with other common traditional training methods, in which data is fed randomly. Studies have shown that curriculum learning can improve generalization, reduce overfitting, and lead to faster convergence, especially in complex tasks like natural language processing or computer vision <a href="#ref_7">[7]</a>.<br><br>

                    Additionally, there have also been many recent advancements in image classification with limited training examples. Feature disentanglement in the context of few-shot image classification focuses on isolating independent features, such as attributes and objects to generalize effectively to unseen combinations. For instance, a method proposed by Xu et al. decomposes input features into class-specific and intra-class variance components. The probabilistic modeling of intra-class variance allows the generation of augmented features that help improve classifier performance on few-shot tasks, especially in those conditions where data diversity is at a minimum <a href="#ref_8">[8]</a>. <br><br>

                    Our new optimization technique, ADS, draws inspiration from CL, Adam, and few-shot learning techniques. The iterative process of ADS differs from CL in the sense that it may be trained on the same data multiple times throughout the entire training process and is not necessarily presented in an “increasing order of difficulty”.  However, whereas CL preselects the order the model sees the training data, ADS dynamically selects the classes the model is performing the worst at during the training process and targets optimization toward those classes using the Adam optimizer.

				  
		    </div>
		    <div class="margin-right-block"> <!-- you can move the margin notes up and down with translate -->
		    </div>
		</div>

		<div class="content-margin-container", id="methods_experiments">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
            <h1>Methods and Experiments</h1>
            The large image dataset our initial ViT is trained on is the ImageNet dataset consisting of over 1.2 million training images and 50k validation images, each of which has a resolution of 469 x 387. This pretrained ViT was taken from <a href="https://huggingface.co/google/vit-base-patch16-224?library=transformers">here</a>. For transfer learning, we used smaller image datasets such as the <b>Food101</b> dataset (101 classes, 1000 images per class), the <b> Oxford Flowers </b> dataset (102 classes, 40 and 258 images per class), and the <b>Describable Textures</b> Dataset (47 classes, 120 images per classes). Our experiments were structured around comparing baseline training runs against our modified training schemes designed to maximize performance on transferring to these smaller datasets. <br><br>

            <i>Baseline Training:</i><br><br>
            For our baseline experiments, we ran the following two training frameworks:
            <ol>
                <li><b>No-freeze Training:</b> The entire model, including the encoder and classifier head, was left unfrozen and was finetuned on the smaller datasets until convergence.</li>
                <li><b>All-freeze Training:</b> All parts of the model had their weights frozen except for the classifier head and the embedding weights, which were fine-tuned on the smaller datasets until convergence.</li>
            </ol>
            The motivation behind freezing and no freezing is that we hope to see that with less tunable parameters during finetuning, the model will be forced to learn “efficiently”, updating its weights in a more precise manner than if it had access to changing the weights of the entire model. This should theoretically lead to faster convergence in terms of total data points processed. <br><br>

            <i>Dynamic Training:</i><br><br>
			
			<img src="./figures/cheezbuga.png" width=600px/>

            To improve the convergence rate and decrease the amount of computation needed to converge, we explored using <b>targeted epochs</b>. These targeted training epochs involve modifying the distribution of training samples by increasing the representation of classes that the model performed poorly on during the last epoch's validation. The key steps for determining the mixture of labels within a targeted epoch were as follows:

            <ol>
                <li>
                    <strong>Class Accuracies:</strong> During the validation on the full dataset mid-training (at predefined intervals of <i>n</i> epochs, bullet point 2 for more details), we track the per-class accuracy of the model to identify the “bad classes”; these were classes that the model struggled to classify correctly and exhibited the worst overall performance on.
                </li>
                <li>
                    <strong>Targeted Epoch Frequency:</strong> We varied the rate at which the training would encounter a targeted epoch to ensure optimal performance.
                    <ol type="a">
                        <li>
                            <strong>Alternative Training Regimens:</strong> We explored two different methods of mixing in the targeted and general training epochs:
                            <ol type="i">
                                <li>
                                    <strong>Alternating Phases (AP):</strong> Training was performed for a fixed number of epochs on the entire dataset, followed by a fixed number of targeted epochs. This alternation repeated throughout training. In other words, if we let a general training epoch = G and a targeted training epoch = T, this follows the pattern (for <i>n</i> = 3): <strong>GGGTGGTGG</strong>… Generalizing, this approach follows the pattern of:
                                </li>
                                <u>Pattern Formula:</u>
                                <p>The pattern alternates between a block of <strong>G</strong> epochs (general training) and a block of <strong>T</strong> epochs (targeted training). The number of <strong>G</strong> epochs decreases incrementally after the first block.</p>

                                <p>
                                    \[
                                    \text{Pattern}(n) = \underbrace{G G \ldots G}_{n \text{ times}} \, T \, 
                                    \underbrace{G G \ldots G}_{n-1 \text{ times}} \, T \, 
                                    \underbrace{G G \ldots G}_{n-1 \text{ times}} \, T \, \ldots
                                    \]
                                </p>

                                Examples:
                                <ul>
                                    <li>
                                        \( n = 2: \, GG \, T \, G \, T \, G \, T \ldots \)
                                    </li>
                                    <li>
                                        \( n = 3: \, GGG \, T \, GG \, T \, GG \, T \ldots \)
                                    </li>
                                    <li>
                                        \( n = 4: \, GGGG \, T \, GGG \, T \, GGG \, T \ldots \)
                                    </li>
                                </ul>
                                
                                <li>
                                    <strong>Transition to Only Targeted Epochs (TOTE):</strong> Training was performed for a fixed number of epochs on the entire dataset. Then, training shifts entirely to a dynamically mixed dataset <em>with an emphasis on the bad classes</em>, using updated mixtures determined periodically through validation. Below is an example of what the training order might look like (for <i>n</i> = 3): <strong>GGGTTT|TTT|TTT</strong>…, where a vertical bar represents updating the dynamic mixture. In this pattern, the training transitions entirely to a dynamically mixed dataset with an emphasis on the bad classes, using updated mixtures determined periodically through validation. Each update is represented by a vertical bar <code>|</code>. Generalizing, this approach follows the pattern of:
                                </li>

                                <u>Pattern Formula:</u>
                                <p>
                                    \[
                                    \text{Pattern}(n) = \underbrace{G G \ldots G}_{n \text{ times}} \, 
                                    \underbrace{T T \ldots T}_{n \text{ times}} \, \mid \, 
                                    \underbrace{T T \ldots T}_{n \text{ times}} \, \mid \, 
                                    \underbrace{T T \ldots T}_{n \text{ times}} \, \ldots
                                    \]
                                </p>

                                Examples:
                                <ul>
                                    <li>
                                        \( n = 2: \, GGTT \, \mid \, TT \, \mid \, TT \, \mid \, TT \ldots \)
                                    </li>
                                    <li>
                                        \( n = 3: \, GGGTTT \, \mid \, TTT \, \mid \, TTT \ldots \)
                                    </li>
                                    <li>
                                        \( n = 4: \, GGGGTTTT \, \mid \, TTTT \, \mid \, TTTT \ldots \)
                                    </li>
                                </ul>
                            </ol>
                        </li>
                    </ol>
                <li>
                    <b>Top-k Bad Classes:</b> We selected the top <em>k</em> classes with the lowest accuracy to emphasize in subsequent epochs. 
            This varied from between 5% → 50% of the total number of samples in each dataset.
                </li>
                <li>
                    <b>Distribution of Classes:</b> We adjusted the proportion of samples per class in the training dataset to over-represent these bad classes while maintaining a balanced representation of the remaining classes. 
            This was explored with weights of \(w = [0.6, 0.8, 1]\), where 1 represents only mixing in the bad classes and 0 approaches vanilla training. These weights refer to the proportion of samples in the epoch that are from the "bad classes". Meanwhile, the remaining classes are weighted so that they comprise \(p = 1-w\) proportion of the total samples in that epoch.
                </li>
                   
                </ol>

                </li>
                </li>
            </ol>

            Overall, we expect that varying these training hyperparameters will provide differing levels of performance, but hopefully allow for a tunable approach to trading-off accuracy for computational usage. <br><br>
            

            <i>Scratch Training:</i><br><br>
            Finally, we also explore training a ViT model from scratch. We compare using the ADS framework for training against vanilla training, hopefully seeing improvements in convergence rates due to the addition of targeted epochs. Due to compute and budget constraints this is only conducted on the Food101 dataset.<br><br>

		    </div>
		    <div class="margin-right-block" style="transform: translate(0%, -250%);">
				<b>Figure 2.</b> The ADS training scheme with alternating phases. Every \(n\) batches (shown with \(n=3\)), the training dataset is replaced by a reweighted dataset where the classes that the model achieves lowest accuracy on during validation are weighted higher than the classes that the model achieves higher accuracy on.
		    </div>
		</div>

		<div class="content-margin-container" id="implementation">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>Technical Details</h1>
						<p><b>Model training.</b>&nbsp;&nbsp;&nbsp;We optimize all reward models with AdamW using an initial learning rate of $10^{-5}$, a batch size of 8, and a single epoch over the full UltraFeedback training split. Among the checkpoints saved during training, we select the one with the highest validation accuracy on a held-out subset of UltraFeedback. GRPO fine-tuning used AdamW with an initial learning rate of $10^{-5}$, run for one epoch on 17,068 prompts from the UltraChat dataset (1/15 of the train split due to compute constraints) <a href="#ref_18">[18]</a>. Due to compute/memory constraints, we fine-tuned the policy using parameter-efficient Low-Rank Adaptation (LoRA) on top of the frozen base model. We enabled LoRA only on the attention query and value projection layers in each transformer block. The LoRA rank was set to 16 with a scaling factor of 32, yielding adaptation scale 2. A dropout rate of 0.05 was applied on the LoRA branch to regularize the adapters.</p>
						<p><b>Hyperparameter Sweep.</b>&nbsp;&nbsp;&nbsp;For GCE and SCE, we perform a hyperparameter search on each level of noise, and evaluate on the test split of UltraFeedback. To tune the GCE $q$ hyperparameter, we run a small grid search over $q \in \{0.3, 0.5, 0.7, 0.9, 1\}$. For SCE, we search over four $(\alpha, \beta)$ pairs: $\{ (0.1, 1.0), (0.5, 1.0), (0.7, 0.7), (1.5, 0.5)\}$. Note that these pairs span a wide range of $\alpha /\beta$ ratios, so that we test settings where the reverse or standard cross-entropy term dominates, as well as more balanced settings. After the hyperparameter search, the best $q$ and $(\alpha, \beta)$ are chosen separately for each noise level while other hyperparameters are kept identical to ensure a fair comparison between BT, GCE, and SCE.</p>
						<p><b>Compute Details.</b>&nbsp;&nbsp;&nbsp;All experiments are implemented in Python using the Hugging Face transformers and Transformer Reinforcement Learning (TRL) libraries together with PyTorch <a href="#ref_20">[20</a>, <a href="#ref_21">21]</a>. Reward models were trained on 8 H100 GPUs averaging approximately 1 hour per reward model run. GRPO was trained on 32 H100 GPUs averaging approximately 45 hours per run.</p>
		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

        <div class="content-margin-container" id="discussion">
            <div class="margin-left-block">
            </div>
        <div class="main-content-block">
                    <h1>Discussion</h1>
                    <h2 id="reward-model-training-results">Reward Model Training Results</h2>
					<img src="./figures/rm_results_table.png" width="450px" style="padding-top: 10px;"/>
					<p>Our standalone reward model experiments demonstrate how robust losses behave under increasing label noise. As the noise level increases, all losses result in a drop in test accuracy on the clean UltraFeedback data. However, the rate of degradation differs across the different loss functions. For each noise level, we analyze the results considering the best hyperparameters for GCE and SCE below.</p>
					<p><b>Clean data.</b>&nbsp;&nbsp;&nbsp;When training on clean data ($p=0$), all three losses achieve very similar performance: BT at 75.23%, GCE at 75.45%, and SCE at 75.92% accuracy. Interestingly, this suggests that using GCE and SCE can match BT performance on clean data if the hyperparameters are chosen carefully.</p>
					<p><b>Moderate noise.</b>&nbsp;&nbsp;&nbsp;Under moderate noise ($p=0.2$), the BT baseline remains surprisingly competitive. Its accuracy drops to 73.93%, GCE drops to 73.81%, and SCE drops to 73.48%. These differences in accuracy are again very small, and may only be attributed to variance in training and evaluation. Nonetheless, at this noise level, BT loss actually achieves the best test accuracy. This suggests that for reasonably high-quality preference data, with noise on the order of 20%, the three losses are quite comparable. Simply training a BT reward model is still quite effective, and the benefits of swapping in a robust loss like GCE or SCE are limited.</p>
					<p><b>High noise.</b>&nbsp;&nbsp;&nbsp;Finally, under the highest noise setting ($p=0.4$), the benefits of the robust losses become apparent. The BT baseline degrades the fastest, while GCE and SCE result in a slower drop in performance. Specifically, BT accuracy falls to 67.52%, while GCE retains 69.94% accuracy, and SCE achieves 69.63%. So, at high noise levels, the robust losses recover up to around $2.5$ percentage points of accuracy over BT, indicating that they can extract a non-trivial amount of signal that BT loss fails to exploit. In the next section, we further run downstream policy optimization using these models to examine if this performance gap affects the policy meaningfully.</p>
					<img src="./figures/rm_acc.png" width="750px" style="padding-top: 10px;"/>
					<h2 id="grpo-results">GRPO Results</h2>
					<p>We next evaluate whether improvements in reward model robustness translate into better downstream policies under GRPO. Using the setup described in Methods, we compare two policies that trained on the noisiest data ($p=0.4$): one fine-tuned with a reward model trained using the baseline BT loss, and another fine-tuned with a reward model trained using GCE with $q=0.9$. The rewards and GRPO objective during training are shown below.</p>
					<img src="./figures/grpo_train_curve.png" width="700px" style="padding-top: 10px;"/>
					<p>Our results show that the GCE-0.9-based policy substantially outperforms the BT-based policy. The GCE-0.9 policy is preferred by the LM judge on 707/1,024 UltraChat prompts (69.04%) and on 542/750 Arena-Hard-Auto prompts (72.26%). For both datasets, the GCE robust‑loss policy wins more than twice as often as the baseline. This gap is far larger than would be expected from random variation, suggesting that the choice of loss for reward modeling can have a significant downstream impact on GRPO under noisy preference data.</p>
					<p>On UltraChat, these results shows that a more noise-robust reward model yields substantially better behavior on the training domain. On Arena-Hard-Auto, the high win rate suggests that this performance extends to difficult, real-world questions and creative writing, as opposed to overfitting to the GRPO training data. Together with the standalone reward‑model results, these findings suggest that even relatively modest gains in reward robustness can lead to substantial improvements in alignment quality.</p>
					<p>We hypothesize that this improvement can be attributed to both the higher RM accuracy, as well as potentially <b>greater reward margins</b> by the robust loss functions. Specifically, in addition to gaining 2.5% in eval accuracy, the GCE-0.9 reward model outputted reward scores that were significantly more spread out than that of the BT reward model. We calculate the evaluation margin of the best reward models at each setting to confirm this, which is the average difference between the reward given on the preferred response and the rejected response over the evaluation set. We report these metrics below over all noise levels and loss functions at the best hyperparameters. We observe that GCE and SCE consistently output higher reward margin.</p>
					<img src="./figures/rm_eval_margin_table.png" width="400px" style="padding-top: 10px;"/>
				</div>
        <div class="margin-right-block" style="transform: translate(0%, -250%);">
			<p style="transform: translate(0%, 0%);">
				<b>Table 1.</b> Reward model accuracy (%) over loss functions and noise levels.
			</p>
			<p style="transform: translate(0%, 1600%);">
				<b>Figure 1.</b> Train and eval curves for best reward models.
			</p>
			<p style="transform: translate(0%, 2800%);">
				<b>Figure 2.</b> Train reward and objective curves during GRPO training.
			</p>
		</div>
    </div>

    <div class="content-margin-container" id="conclusion">
        <div class="margin-left-block">
        </div>
    <div class="main-content-block">
                <h1>Conclusion</h1>
                <p>We studied robust reward modeling for RLHF under noisy preference data by analyzing the effect of loss functions from the noisy-label literature. Specifically, we compared training reward models with the standard Bradley-Terry (BT) loss against Generalized Cross Entropy (GCE) and Symmetric Cross Entropy (SCE) on UltraFeedback under synthetic noise levels \(p\in \{0, 0.2, 0.4\}\). Then, we evaluated the downstream GRPO policies from high-performing reward models under high-noise conditions. Our results show that robust losses behave similarly to BT for clean or moderately noisy data, but offer substantial improvement when label corruption becomes severe. At \(p=0.4\), both GCE and SCE degrade more slowly than BT, yielding an additional 2-2.5 percentage points of reward-model accuracy.</p>
				<p>Notably, these modest accuracy gains translate into large improvements after policy optimization. When we fine-tune Qwen3-0.6B-Base with GRPO using the BT reward model vs. the best robust variant (GCE with \(q=0.9\)), the GCE-based policy is preferred by an LM judge on 69.04% on UltraChat testing prompts and 72.26% Arena-Hard-Auto prompts. Our results suggest that robust losses such as GCE and SCE are simple, drop-in replacements for BT that can make alignment training more robust under high noise settings.</p>
				<h2 id="limitations">Limitations</h2>
				<p>Our study has several limitations that point to directions for future work. First, our noise model is relatively simple, capturing only symmetric random label corruption. In practice, noise is often structured (e.g., length bias, annotator biases, etc.), and it is unclear how well our conclusions transfer to these more complex settings. Second, due to computational constraints, we evaluate on a single preference dataset (UltraFeedback) with one relatively small base model (Qwen3-0.6B-Base). The performance of robust losses may differ for larger models and other domains. We also only trained GRPO on two models at 1/15 of the full dataset due to compute constraints, so further evaluations here could show interesting trends. Finally, we focus only on two prominent losses from the noisy-label literature, leaving many other noise-robust techniques unexplored. Thus, future work could (i) study robust losses under more realistic noise and on multiple RLHF datasets, (ii) evaluate larger and different base models with more data, and (iii) analyze additional robust losses from the noisy-label literature.</p>

    </div>
    <div class="margin-right-block">
    </div>
</div>

		<div class="content-margin-container" id="citations">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<div class='citation' id="references" style="height:auto"><br>
							<span style="font-size:16px">References:</span><br><br>
							<a id="ref_1"></a>[1] <a href="https://arxiv.org/abs/2010.11929">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a>, Dosovitskiy et al., 2020<br><br>
							<a id="ref_2"></a>[2] <a href="https://arxiv.org/pdf/2405.14813">Scalable Optimization in the Modular Norm</a>, Large et al., 2024<br><br>
							<a id="ref_3"></a>[3] <a href="https://arxiv.org/abs/1412.6980">Adam: A Method for Stochastic Optimization</a>, Kingma et al., 2014<br><br>
							<a id="ref_4"></a>[4] <a href="https://arxiv.org/abs/2208.09632">Adam Can Converge Without Any Modification On Update Rules</a>, Zhang et al., 2022<br><br>
							<a id="ref_5"></a>[5] <a href="https://arxiv.org/abs/2311.05589">A Coefficient Makes SVRG Effective</a>, Yin et al., 2023<br><br>
							<a id="ref_6"></a>[6] <a href="https://www.sciencedirect.com/science/article/pii/0010027793900584">Learning and development in neural networks: the importance of starting small</a>, Elman, 1993<br><br>
							<a id="ref_7"></a>[7] <a href="https://arxiv.org/abs/1808.01097">CurriculumNet: Weakly Supervised Learning from Large-Scale Web Images</a>, Guo et al., 2018<br><br>
							<a id="ref_8"></a>[8] <a href="https://arxiv.org/abs/2010.03255">Variational Feature Disentangling for Fine-Grained Few-Shot Classification</a>, Xu et al., 201<br><br>
						</div>
		    </div>
		    <div class="margin-right-block">
            <!-- margin notes for reference block here -->
		    </div>
		</div>

	</body>

</html>